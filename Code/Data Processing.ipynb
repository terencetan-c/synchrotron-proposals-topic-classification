{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing publications\n",
    "\n",
    "The publications from each facility aare imported.\\\n",
    "The publications were downloaded from the databases of the various facilities.\\\n",
    "For HZB, an API call is used to download the publications\\\n",
    "\n",
    "ESRF/ILL: https://epn-library.esrf.fr/ \\\n",
    "DLS: https://publications.diamond.ac.uk/ \\\n",
    "HZB: https://www.helmholtz-berlin.de/pubbin/publikationen.pl \\\n",
    "\n",
    "Only the ESRF section of the code is complete; the rest are work in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of ESRF publications. We shall designate them as follows:\n",
    "<br>\n",
    "Type 1: Publications with ESRF authors and describing ESRF experiments\n",
    "<br>\n",
    "Type 2: Publications without ESRF authors and describing ESRF experiments\n",
    "<br>\n",
    "Type 3: Publications with ESRF authors and not describing ESRF experiments\n",
    "<br>\n",
    "Type 4: Articles citing the ESRF, no ESRF authors\n",
    "<br>\n",
    "This 'Type' information willl be included alongside the other metadata in the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all ESRF publication files\n",
    "pub_esrf_type1=pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Raw_data/ESRF publications with ESRF authors and describing ESRF experiment (Oct 2024).csv',sep='\t', encoding='utf-8',skiprows=1,header=0)    # Import file\n",
    "pub_esrf_type2=pd.concat([pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Raw_data/ESRF Publications without ESRF authors and describing an ESRF experiment (2015 onwards).csv',sep='\t', encoding='utf-8',skiprows=1,header=0),pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Raw_data/ESRF Publications without ESRF authors and describing an ESRF experiment (bef 2015).csv',sep='\t', encoding='utf-8',skiprows=1,header=0)],ignore_index=True)    # Import file\n",
    "pub_esrf_type3=pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Raw_data/ESRF Publications with ESRF authors and not describing an ESRF experiment.csv',sep='\t', encoding='utf-8',skiprows=1,header=0)    # Import file\n",
    "pub_esrf_type4=pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Raw_data/ESRF articles citing ESRF, no ESRF author.csv',sep='\t', encoding='utf-8',skiprows=1,header=0)    # Import file\n",
    "\n",
    "# Add 'Type' column\n",
    "pub_esrf_type1['Type']=1\n",
    "pub_esrf_type2['Type']=2\n",
    "pub_esrf_type3['Type']=3\n",
    "pub_esrf_type4['Type']=4\n",
    "\n",
    "# Concatenate all four types\n",
    "pub_esrf=pd.concat([pub_esrf_type1,pub_esrf_type2,pub_esrf_type3,pub_esrf_type4],ignore_index=True)\n",
    "\n",
    "# Add 'Facility repository' column\n",
    "pub_esrf['Facility repository']='ESRF'\n",
    "\n",
    "# Uppercase all the proposal names\n",
    "pub_esrf['Proposal number'] = pub_esrf['Proposal number'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to csv file\n",
    "pub_esrf.to_csv('{insert path here}/Publications_ESRF',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of ILL publications. We shall designate them as follows:\n",
    "<br>\n",
    "Type 1: Publications with ILL authors and describing ILL experiments\n",
    "<br>\n",
    "Type 2: Publications without ILL authors and describing ILL experiments\n",
    "<br>\n",
    "Type 3: Publications with ILL authors and not describing ILL experiments\n",
    "<br>\n",
    "Type 4: Articles citing the ILL, no ILL authors\n",
    "<br>\n",
    "This 'Type' information willl be included alongside the other metadata in the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all ILL publication files\n",
    "pub_ill_type1=pd.read_csv('{insert path here}/Datasets/ILL/Publications with ILL authors and describing ILL experiment.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "pub_ill_type2=pd.read_csv('{insert path here}/Datasets/ILL/Publications without ILL author and describing an ILL experiment.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "pub_ill_type3=pd.read_csv('{insert path here}/Datasets/ILL/Publications with ILL authors and not describing ILL experiment.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "pub_ill_type4=pd.read_csv('{insert path here}/Datasets/ILL/Articles citing the ILL, no ILL author.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "\n",
    "# Add 'Type' column\n",
    "pub_ill_type1['Type']=1\n",
    "pub_ill_type2['Type']=2\n",
    "pub_ill_type3['Type']=3\n",
    "pub_ill_type4['Type']=4\n",
    "\n",
    "# Concatenate all four types\n",
    "pub_ill=pd.concat([pub_ill_type1,pub_ill_type2,pub_ill_type3,pub_ill_type4],ignore_index=True)\n",
    "\n",
    "# Add 'Facility repository' column\n",
    "pub_ill['Facility repository']='ILL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and concat all DLS publications\n",
    "pub_dls=pd.concat([pd.read_csv('{insert path here}/Datasets/DLS/DLS annual review highlight.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS book chapter.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS conference paper.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS editor note.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS journal paper (2002-2010).csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS journal paper (2011-2020).csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS journal paper (2021-2024).csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS magazine article.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS poster.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS report.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS science highlight.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS thesis.csv')],ignore_index=True)\n",
    "\n",
    "# Add 'Facility repository' column\n",
    "pub_dls['Facility repository']='DLS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HZB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for metadata in year 1981...\n",
      "Metadata for year 1981 saved to Datasets/HZB/metadata_1981.txt\n",
      "Searching for metadata in year 1982...\n",
      "Metadata for year 1982 saved to Datasets/HZB/metadata_1982.txt\n",
      "Searching for metadata in year 1983...\n",
      "Metadata for year 1983 saved to Datasets/HZB/metadata_1983.txt\n",
      "Searching for metadata in year 1984...\n",
      "Metadata for year 1984 saved to Datasets/HZB/metadata_1984.txt\n",
      "Searching for metadata in year 1985...\n",
      "Metadata for year 1985 saved to Datasets/HZB/metadata_1985.txt\n",
      "Searching for metadata in year 1986...\n",
      "Metadata for year 1986 saved to Datasets/HZB/metadata_1986.txt\n",
      "Searching for metadata in year 1987...\n",
      "Metadata for year 1987 saved to Datasets/HZB/metadata_1987.txt\n",
      "Searching for metadata in year 1988...\n",
      "Metadata for year 1988 saved to Datasets/HZB/metadata_1988.txt\n",
      "Searching for metadata in year 1989...\n",
      "Metadata for year 1989 saved to Datasets/HZB/metadata_1989.txt\n",
      "Searching for metadata in year 1990...\n",
      "Metadata for year 1990 saved to Datasets/HZB/metadata_1990.txt\n",
      "Searching for metadata in year 1991...\n",
      "Metadata for year 1991 saved to Datasets/HZB/metadata_1991.txt\n",
      "Searching for metadata in year 1992...\n",
      "Metadata for year 1992 saved to Datasets/HZB/metadata_1992.txt\n",
      "Searching for metadata in year 1993...\n",
      "Metadata for year 1993 saved to Datasets/HZB/metadata_1993.txt\n",
      "Searching for metadata in year 1994...\n",
      "Metadata for year 1994 saved to Datasets/HZB/metadata_1994.txt\n",
      "Searching for metadata in year 1995...\n",
      "Metadata for year 1995 saved to Datasets/HZB/metadata_1995.txt\n",
      "Searching for metadata in year 1996...\n",
      "Metadata for year 1996 saved to Datasets/HZB/metadata_1996.txt\n",
      "Searching for metadata in year 1997...\n",
      "Metadata for year 1997 saved to Datasets/HZB/metadata_1997.txt\n",
      "Searching for metadata in year 1998...\n",
      "Metadata for year 1998 saved to Datasets/HZB/metadata_1998.txt\n",
      "Searching for metadata in year 1999...\n",
      "Metadata for year 1999 saved to Datasets/HZB/metadata_1999.txt\n",
      "Searching for metadata in year 2000...\n",
      "Metadata for year 2000 saved to Datasets/HZB/metadata_2000.txt\n",
      "Searching for metadata in year 2001...\n",
      "Metadata for year 2001 saved to Datasets/HZB/metadata_2001.txt\n",
      "Searching for metadata in year 2002...\n",
      "Metadata for year 2002 saved to Datasets/HZB/metadata_2002.txt\n",
      "Searching for metadata in year 2003...\n",
      "Metadata for year 2003 saved to Datasets/HZB/metadata_2003.txt\n",
      "Searching for metadata in year 2004...\n",
      "Metadata for year 2004 saved to Datasets/HZB/metadata_2004.txt\n",
      "Searching for metadata in year 2005...\n",
      "Metadata for year 2005 saved to Datasets/HZB/metadata_2005.txt\n",
      "Searching for metadata in year 2006...\n",
      "Metadata for year 2006 saved to Datasets/HZB/metadata_2006.txt\n",
      "Searching for metadata in year 2007...\n",
      "Metadata for year 2007 saved to Datasets/HZB/metadata_2007.txt\n",
      "Searching for metadata in year 2008...\n",
      "Metadata for year 2008 saved to Datasets/HZB/metadata_2008.txt\n",
      "Searching for metadata in year 2009...\n",
      "Metadata for year 2009 saved to Datasets/HZB/metadata_2009.txt\n",
      "Searching for metadata in year 2010...\n",
      "Metadata for year 2010 saved to Datasets/HZB/metadata_2010.txt\n",
      "Searching for metadata in year 2011...\n",
      "Metadata for year 2011 saved to Datasets/HZB/metadata_2011.txt\n",
      "Searching for metadata in year 2012...\n",
      "Metadata for year 2012 saved to Datasets/HZB/metadata_2012.txt\n",
      "Searching for metadata in year 2013...\n",
      "Metadata for year 2013 saved to Datasets/HZB/metadata_2013.txt\n",
      "Searching for metadata in year 2014...\n",
      "Metadata for year 2014 saved to Datasets/HZB/metadata_2014.txt\n",
      "Searching for metadata in year 2015...\n",
      "Metadata for year 2015 saved to Datasets/HZB/metadata_2015.txt\n",
      "Searching for metadata in year 2016...\n",
      "Metadata for year 2016 saved to Datasets/HZB/metadata_2016.txt\n",
      "Searching for metadata in year 2017...\n",
      "Metadata for year 2017 saved to Datasets/HZB/metadata_2017.txt\n",
      "Searching for metadata in year 2018...\n",
      "Metadata for year 2018 saved to Datasets/HZB/metadata_2018.txt\n",
      "Searching for metadata in year 2019...\n",
      "Metadata for year 2019 saved to Datasets/HZB/metadata_2019.txt\n",
      "Searching for metadata in year 2020...\n",
      "Metadata for year 2020 saved to Datasets/HZB/metadata_2020.txt\n",
      "Searching for metadata in year 2021...\n",
      "Metadata for year 2021 saved to Datasets/HZB/metadata_2021.txt\n",
      "Searching for metadata in year 2022...\n",
      "Metadata for year 2022 saved to Datasets/HZB/metadata_2022.txt\n",
      "Searching for metadata in year 2023...\n",
      "Metadata for year 2023 saved to Datasets/HZB/metadata_2023.txt\n",
      "Searching for metadata in year 2024...\n",
      "Metadata for year 2024 saved to Datasets/HZB/metadata_2024.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Base URL for the form submission\n",
    "# search_url = \"https://www.helmholtz-berlin.de/pubbin/publikationen.pl\"\n",
    "# search_url = \"https://www.helmholtz-berlin.de/pubbin/publikationen.pl\"\n",
    "\n",
    "# Years for which to download metadata\n",
    "years = list(range(1981, 2025))  # Adjust the range of years as needed\n",
    "\n",
    "# Directory to save metadata\n",
    "save_directory = 'Datasets/HZB'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Function to extract metadata for a specific year\n",
    "def extract_metadata_by_year(year):\n",
    "    print(f\"Searching for metadata in year {year}...\")\n",
    "    \n",
    "    # Form data to submit for the search\n",
    "    form_data = {\n",
    "        'jahr': str(year),  # Year\n",
    "        'JOB': 'start search',  # Start the search\n",
    "        'sprache':'en',\n",
    "        'typ_1':'1',\n",
    "        'typ_2':'1',\n",
    "        'typ_3':'1',\n",
    "        'typ_5':'1',\n",
    "    }\n",
    "\n",
    "    # Submit the form with a POST request\n",
    "    response = requests.post(search_url, data=form_data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Failed to retrieve results for year {year}\")\n",
    "        return\n",
    "\n",
    "    # Parse the response HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the metadata section (you'll need to inspect the HTML structure to find the right tag)\n",
    "    # For example, it might be in a specific <div>, <table>, or other tag\n",
    "    metadata_text = soup.get_text()  # Get all the text on the page\n",
    "    # Alternatively, you might want to focus on specific areas using soup.find() or soup.select()\n",
    "\n",
    "    # Save the metadata to a text file\n",
    "    metadata_filename = os.path.join(save_directory, f'metadata_{year}.txt')\n",
    "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(metadata_text)\n",
    "    \n",
    "    print(f\"Metadata for year {year} saved to {metadata_filename}\")\n",
    "\n",
    "# Main loop to iterate over years\n",
    "for year in years:\n",
    "    extract_metadata_by_year(year)\n",
    "    time.sleep(5)  # Add a delay between requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract relevant information from a text file\n",
    "def extract_publications(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Regex pattern to capture text between 'select none' and 'Export as'\n",
    "    pattern = re.compile(r'select none(.*?)Export as', re.DOTALL)\n",
    "\n",
    "    # Search for the pattern\n",
    "    match = pattern.search(content)\n",
    "\n",
    "    if match:\n",
    "        # Extract the relevant section\n",
    "        relevant_section = match.group(1)\n",
    "\n",
    "        # Clean up the extracted section by stripping unnecessary whitespace\n",
    "        relevant_section = relevant_section.strip()\n",
    "\n",
    "        # # Remove multiple blank lines using regex\n",
    "        # relevant_section = re.sub(r'\\n\\s*\\n+', '\\n', relevant_section)\n",
    "\n",
    "        # Split the cleaned section into lines\n",
    "        lines = relevant_section.splitlines()\n",
    "\n",
    "        # publications = []\n",
    "        # current_entry = []\n",
    "\n",
    "        # # Process each line to group entries\n",
    "        # for line in lines:\n",
    "        #     stripped_line = line.strip()\n",
    "        #     if stripped_line:  # Only process non-empty lines\n",
    "        #         current_entry.append(stripped_line)\n",
    "\n",
    "        #         # Heuristic: Check if the line indicates the end of an entry\n",
    "        #         # For example, if it contains a DOI or a specific pattern.\n",
    "        #         if re.search(r'\\d{4}', stripped_line) or \"doi:\" in stripped_line.lower():\n",
    "        #             # This assumes the entry ends after a line containing a year or DOI\n",
    "        #             publications.append(\" \".join(current_entry))\n",
    "        #             current_entry = []  # Reset for the next entry\n",
    "\n",
    "        # # Handle any remaining lines in current_entry\n",
    "        # if current_entry:\n",
    "        #     publications.append(\" \".join(current_entry))\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    return lines\n",
    "        \n",
    "\n",
    "# file_path = 'Datasets/HZB/metadata_2016.txt'  # Replace with your file path\n",
    "# lines = extract_publications(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all HZB files\n",
    "publications_hzb=[]\n",
    "for i in range(1981,2025):\n",
    "    file_path = 'Datasets/HZB/metadata_'+str(i)+'.txt'  # Replace with your file path\n",
    "    lines = extract_publications(file_path)\n",
    "    publications_hzb=publications_hzb+lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all DOIs from publications_hzb \n",
    "doi_pattern=r\"doi:\\s(?P<doi>\\S+)(?=Open|$)\"   # DOI starts with \"doi:\" followed by a string\n",
    "hzb_doi=[]\n",
    "for entry in publications_hzb:\n",
    "    doi=re.search(doi_pattern,entry)\n",
    "    if doi:\n",
    "        hzb_doi.append(doi.group(1))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Proposals \n",
    "Now, that we have the publications, the next step is to get the proposals.\\\n",
    "For ESRF, this can be done through the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESRF Proposals\n",
    "The ESRF publications imported above are linked to their corresponding proposals through the proposal number.\\\n",
    "We want to get a complete list of ESRF proposals and map them to the corresponding papers.\\\n",
    "https://icatplus.esrf.fr/api/Documents provides a list of experimental sessions, which can then be mapped to the associated proposals and them to the assosicated publications.\\\n",
    "Note that multiple experimental sessions can be associated with a single proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ESRF publications\n",
    "\n",
    "# pub_esrf=pd.read_csv('{insert pathname}/Datasets/ESRF/Publications_ESRF')    # Import file\n",
    "pub_esrf=pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Publications_ESRF')    # Import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any publications without DOIs\n",
    "pub_esrf=pub_esrf[pub_esrf['DOI'].notna()]\n",
    "\n",
    "# Replace any NaN proposal names with empty string\n",
    "pub_esrf['Proposal number']=pub_esrf['Proposal number'].fillna('')\n",
    "\n",
    "# Convert all proposal names to uppercase\n",
    "pub_esrf['Proposal number']=pub_esrf['Proposal number'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experimental session documents\n",
    "api_request=\"https://icatplus.esrf.fr/api/Documents/\"\n",
    "response=requests.get(api_request).json()\n",
    "documents_esrf=response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DOI, Title, and Summary (abstract) information from documents_esrf into session_esrf\n",
    "# session_esrf will be a list of dicts, with each index in the list representing a proposal; dict keys are: 'doi', 'title', 'summary'\n",
    "\n",
    "session_esrf=[]        # Initialise empty session_esrf list\n",
    "\n",
    "for document in documents_esrf:\n",
    "    session_dict={}        # Initialise empty session_dict dictionary\n",
    "    \n",
    "    # Check for DOI and append those with one\n",
    "    if document['doi']!=None:\n",
    "        session_dict['doi']=document['doi']\n",
    "        session_dict['title']=document['title']\n",
    "        session_dict['summary']=document['summary']\n",
    "        session_esrf.append(session_dict)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '10.15151/ESRF-ES-2171014218',\n",
       " 'title': 'Structural studies of insulin and lysozyme microcrystals',\n",
       " 'summary': 'Structural studies of insulin and lysozyme microcrystals'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_esrf[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(session_esrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8355"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_esrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch Proposal number information of a single session using API call\n",
    "\n",
    "def fetch_data(session):\n",
    "    api_request=\"https://icatplus.esrf.fr/doi/\"\n",
    "    response = requests.get(api_request+session['doi']+'/reports')\n",
    "\n",
    "    # Check for valid API call status code\n",
    "    if response.status_code==200:\n",
    "        reports_esrf=response.json()\n",
    "        proposals=[]\n",
    "        for report in reports_esrf:\n",
    "            proposals.append({report['proposal']:report['reports']})\n",
    "        session['proposal_dict']=proposals\n",
    "        return session     # prop with proposal number added \n",
    "    else:\n",
    "        return 0        # return 0 is API call result is invalid\n",
    "\n",
    "def fetch_subject(session):\n",
    "    api_request=\"https://icatplus.esrf.fr/doi/\"\n",
    "    response2 = requests.get(api_request+session['doi']+'/json-datacite')\n",
    "\n",
    "    # Check for valid API call status code\n",
    "    if response2.status_code==200:\n",
    "        reports2_esrf=response2.json()\n",
    "        if 'subject' in reports2_esrf['subjects'][0]:       # Check if the 'subject' key exists\n",
    "            session['subject']=reports2_esrf['subjects'][0]['subject']\n",
    "        else:\n",
    "            pass\n",
    "        return session\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each session in session_esrf and apply the fetch_data function. Only append session that has valid proposal number to session_esrf_valid\n",
    "\n",
    "session_esrf_valid=[]\n",
    "for session in session_esrf:\n",
    "    fetched_data=fetch_data(session)\n",
    "    if fetched_data!=0:\n",
    "        session_esrf_valid.append(session)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each session in session_esrf_valid and apply the fetch_subject function.\n",
    "\n",
    "session_esrf_valid_with_subj=[]\n",
    "\n",
    "for session in session_esrf_valid:\n",
    "    fetched_data=fetch_subject(session)\n",
    "    if fetched_data!=0:\n",
    "        session_esrf_valid_with_subj.append(session)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list to json format\n",
    "with open('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/session_esrf_valid_with_subj.json', 'w') as f:\n",
    "    json.dump(session_esrf_valid_with_subj, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of session with zero proposals 0\n",
      "Number of session with exactly one proposal 8289\n",
      "Number of session with more than one proposals 0\n"
     ]
    }
   ],
   "source": [
    "# Check how many proposals numbers each session is linked to\n",
    "sum_of_none = 0\n",
    "sum_of_one = 0\n",
    "sum_of_more = 0\n",
    "for session in session_esrf_valid_with_subj:\n",
    "    if len(session['proposal_dict']) == 1:\n",
    "        sum_of_one += 1\n",
    "    elif len(session['proposal_dict']) == o:\n",
    "        sum_of_none += 1\n",
    "    else:\n",
    "        sum_of_more += 1\n",
    "\n",
    "print('Number of session with zero proposals',sum_of_none)\n",
    "print('Number of session with exactly one proposal',sum_of_one)\n",
    "print('Number of session with more than one proposals',sum_of_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like all sessions are linked to only one proposal. In that case, we can separate the proposal number from the pdf document name for each session.\\\n",
    "For example, 'proposal_dict': [{'MA-6393': ['109865_0001.pdf']}] can be split into 'proposal': 'MA-6393' and 'pdf document name': '109865_0001.pdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in session_esrf_valid_with_subj:\n",
    "    session['proposal'] = list(session['proposal_dict'][0].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in session_esrf_valid_with_subj:\n",
    "    session['PDF document name'] = list(session['proposal_dict'][0].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doi': '10.15151/ESRF-ES-118169876', 'title': 'CryoEM determination of the closed inactivated voltage-gated sodium channel to compare structure in different functional states', 'summary': None, 'proposal': 'MX-2002', 'subject': 'Macromolecular Crystallography', 'proposal_dict': [{'MX-2002': ['88850_A.pdf', '88850_B.pdf']}], 'PDF document name': ['88850_A.pdf', '88850_B.pdf']}\n"
     ]
    }
   ],
   "source": [
    "# for session in session_esrf_valid_with_subj:\n",
    "#     if len(session['PDF document name']) >= 2:\n",
    "#         print(session)\n",
    "#         break\n",
    "#     else:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert session_esrf_valid to DataFrame, apply upper case to all proposal names for standardisation, replace all NaN values with blank strings in the 'subject' column.\n",
    "\n",
    "df=pd.DataFrame(session_esrf_valid_with_subj)\n",
    "df['proposal']=df['proposal'].str.upper()\n",
    "df['subject']=df['subject'].fillna('')\n",
    "\n",
    "\n",
    "# Group by proposal, and for each of the 'summary', 'title', 'subject' columns, concatenate unique strings. For 'doi' column, gather all the experiment session DOIs into a list for each proposal.\n",
    "df = df.groupby('proposal').agg({\n",
    "    'summary': lambda x: '. '.join(sorted(set(v for v in x if v))),\n",
    "    'title': lambda x: '. '.join(set(x)),\n",
    "    'subject': lambda x: ', '.join(sorted(set(v for v in x if v))),\n",
    "    'doi': lambda x: list(x.dropna()),\n",
    "    'PDF document name': lambda x: list(set(i for sublist in x for i in sublist if i))\n",
    "}).reset_index()\n",
    "\n",
    "df.rename(columns={'doi':'experiment session doi'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposal</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>subject</th>\n",
       "      <th>experiment session doi</th>\n",
       "      <th>PDF document name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>MX-2440</td>\n",
       "      <td>At the EPN campus level structural biology wor...</td>\n",
       "      <td>Cryo-electron microscopy EPN BAG</td>\n",
       "      <td>Macromolecular Crystallography</td>\n",
       "      <td>[10.15151/ESRF-ES-1118655127, 10.15151/ESRF-ES...</td>\n",
       "      <td>[101905_E.pdf, 101905_A.pdf, 101905_S.pdf, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>MX-2441</td>\n",
       "      <td>This proposal gathers Principal investigators ...</td>\n",
       "      <td>The proposal is a continuation of the MX-2367 ...</td>\n",
       "      <td>Macromolecular Crystallography</td>\n",
       "      <td>[10.15151/ESRF-ES-933397150, 10.15151/ESRF-ES-...</td>\n",
       "      <td>[101906_J.pdf, 101906_C.pdf, 101906_K.pdf, 101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     proposal                                            summary  \\\n",
       "4857  MX-2440  At the EPN campus level structural biology wor...   \n",
       "4858  MX-2441  This proposal gathers Principal investigators ...   \n",
       "\n",
       "                                                  title  \\\n",
       "4857                   Cryo-electron microscopy EPN BAG   \n",
       "4858  The proposal is a continuation of the MX-2367 ...   \n",
       "\n",
       "                             subject  \\\n",
       "4857  Macromolecular Crystallography   \n",
       "4858  Macromolecular Crystallography   \n",
       "\n",
       "                                 experiment session doi  \\\n",
       "4857  [10.15151/ESRF-ES-1118655127, 10.15151/ESRF-ES...   \n",
       "4858  [10.15151/ESRF-ES-933397150, 10.15151/ESRF-ES-...   \n",
       "\n",
       "                                      PDF document name  \n",
       "4857  [101905_E.pdf, 101905_A.pdf, 101905_S.pdf, 101...  \n",
       "4858  [101906_J.pdf, 101906_C.pdf, 101906_K.pdf, 101...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['PDF document name'].str.len()>=11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Regex to map Proposal names in df to Proposal names in pub_esrf, and get a list of publication DOIs for each proposal\n",
    "doi_list=[]\n",
    "for proposal in df['proposal']:\n",
    "    matching_dois=pub_esrf[pub_esrf['Proposal number'].str.contains(r'\\b' + proposal + r'\\b', regex=True,na=False)]['DOI'].dropna().to_list()\n",
    "    doi_list.append(matching_dois)\n",
    "\n",
    "# Append the list of publication DOIs to df under the new column 'publications DOI'\n",
    "df['publications DOI']=doi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to json format\n",
    "df.to_json('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Checkpoint_data/Proposals_ESRF', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OpenAlex citation metadata for proposals\n",
    "Now that we have the proposals with the DOIs of the publications linked to the proposals, we want to use the OpenAlex API call on the DOIs to get the corresponding OpenAlex IDs. This is because the topic classification model takes in OpenAlex IDs and not DOIs as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ESRF proposals\n",
    "prop_esrf=pd.read_json('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Checkpoint_data/Proposals_ESRF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposal</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>subject</th>\n",
       "      <th>experiment session doi</th>\n",
       "      <th>PDF document name</th>\n",
       "      <th>publications DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01-2-1247</td>\n",
       "      <td>The proposal falls within the general research...</td>\n",
       "      <td>Crystal structure of multiferroic KNi_1-xCo_xP...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-670011307]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01-2-1248</td>\n",
       "      <td>Having achieved successful results with metal-...</td>\n",
       "      <td>Metal-organic and covalent organic polyhedra f...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-670011305]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01-2-1249</td>\n",
       "      <td>The overall aim of the project is deciphering ...</td>\n",
       "      <td>The redox structure of haem- and flavoproteins...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-670011413]</td>\n",
       "      <td>[98064_A.pdf]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01-2-1254</td>\n",
       "      <td>In this study, we will investigate structural ...</td>\n",
       "      <td>Nickelates – phase transitions, distortions an...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-748027553]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01-2-1255</td>\n",
       "      <td>We developed a crystallization strategy that p...</td>\n",
       "      <td>Understanding the structure of two-dimensional...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-670011338]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10.1038/s41563-023-01669-z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>XA-11</td>\n",
       "      <td>Steel slag is one of the most common wastes pr...</td>\n",
       "      <td>Time-resolved X-ray Tomography imaging and Ram...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-1647778275]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>XA-5</td>\n",
       "      <td>The aim of this proposal is to elucidate the i...</td>\n",
       "      <td>ReMade Proposal\\r\\nImpact of metals blend and ...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-1424924468]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>XA-6</td>\n",
       "      <td>Recycling spent Li-ion batteries has attracted...</td>\n",
       "      <td>ReMade Proposal\\r\\nOperando investigation stru...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-1436201044]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>XA-7</td>\n",
       "      <td>This proposals combines our expertise in metal...</td>\n",
       "      <td>ReMade Proposal\\r\\nTuning the sorption propert...</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-1352264747]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>XA-9</td>\n",
       "      <td>Aerospace components are susceptible to wear a...</td>\n",
       "      <td>Additive repairing for turbine blisk aerofoils</td>\n",
       "      <td></td>\n",
       "      <td>[10.15151/ESRF-ES-1720462860]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5361 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        proposal                                            summary  \\\n",
       "0     A01-2-1247  The proposal falls within the general research...   \n",
       "1     A01-2-1248  Having achieved successful results with metal-...   \n",
       "2     A01-2-1249  The overall aim of the project is deciphering ...   \n",
       "3     A01-2-1254  In this study, we will investigate structural ...   \n",
       "4     A01-2-1255  We developed a crystallization strategy that p...   \n",
       "...          ...                                                ...   \n",
       "5356       XA-11  Steel slag is one of the most common wastes pr...   \n",
       "5357        XA-5  The aim of this proposal is to elucidate the i...   \n",
       "5358        XA-6  Recycling spent Li-ion batteries has attracted...   \n",
       "5359        XA-7  This proposals combines our expertise in metal...   \n",
       "5360        XA-9  Aerospace components are susceptible to wear a...   \n",
       "\n",
       "                                                  title subject  \\\n",
       "0     Crystal structure of multiferroic KNi_1-xCo_xP...           \n",
       "1     Metal-organic and covalent organic polyhedra f...           \n",
       "2     The redox structure of haem- and flavoproteins...           \n",
       "3     Nickelates – phase transitions, distortions an...           \n",
       "4     Understanding the structure of two-dimensional...           \n",
       "...                                                 ...     ...   \n",
       "5356  Time-resolved X-ray Tomography imaging and Ram...           \n",
       "5357  ReMade Proposal\\r\\nImpact of metals blend and ...           \n",
       "5358  ReMade Proposal\\r\\nOperando investigation stru...           \n",
       "5359  ReMade Proposal\\r\\nTuning the sorption propert...           \n",
       "5360     Additive repairing for turbine blisk aerofoils           \n",
       "\n",
       "             experiment session doi PDF document name  \\\n",
       "0      [10.15151/ESRF-ES-670011307]                []   \n",
       "1      [10.15151/ESRF-ES-670011305]                []   \n",
       "2      [10.15151/ESRF-ES-670011413]     [98064_A.pdf]   \n",
       "3      [10.15151/ESRF-ES-748027553]                []   \n",
       "4      [10.15151/ESRF-ES-670011338]                []   \n",
       "...                             ...               ...   \n",
       "5356  [10.15151/ESRF-ES-1647778275]                []   \n",
       "5357  [10.15151/ESRF-ES-1424924468]                []   \n",
       "5358  [10.15151/ESRF-ES-1436201044]                []   \n",
       "5359  [10.15151/ESRF-ES-1352264747]                []   \n",
       "5360  [10.15151/ESRF-ES-1720462860]                []   \n",
       "\n",
       "                  publications DOI  \n",
       "0                               []  \n",
       "1                               []  \n",
       "2                               []  \n",
       "3                               []  \n",
       "4     [10.1038/s41563-023-01669-z]  \n",
       "...                            ...  \n",
       "5356                            []  \n",
       "5357                            []  \n",
       "5358                            []  \n",
       "5359                            []  \n",
       "5360                            []  \n",
       "\n",
       "[5361 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_esrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, config\n",
    "config.email = 'terence.tan@wadham.ox.ac.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applying the API call to each row of the proposal DataFrame prop_esrf\n",
    "\n",
    "def get_openalex_id(row):\n",
    "    # Get the 'publications DOI' column from the row\n",
    "    publications_doi = row['publications DOI']\n",
    "    openalex_id = []\n",
    "    \n",
    "    for doi in publications_doi:\n",
    "        result = Works().filter(doi=doi).select('id').get()\n",
    "        # If the result is not empty, append the 'id', otherwise do nothing\n",
    "        if result:\n",
    "            openalex_id.append(result[0]['id'])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    row['openalex_ids']=openalex_id\n",
    "   \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the propsals with publications and those without\n",
    "\n",
    "prop_esrf_non_empty=prop_esrf[prop_esrf['publications DOI'].str.len()!=0]\n",
    "prop_esrf_empty=prop_esrf[prop_esrf['publications DOI'].str.len()==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the get_openalex_id to each row of prop_esrf_non_empty\n",
    "prop_esrf_non_empty=prop_esrf_non_empty.apply(get_openalex_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proposals with different number of publications DOI and OpenAlex IDs:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "# Check if the number of publications DOI is equal to the number of OpenAlex IDs for each proposal\n",
    "\n",
    "print(\"Number of proposals with different number of publications DOI and OpenAlex IDs:\")\n",
    "print(len(prop_esrf_non_empty[prop_esrf_non_empty['publications DOI'].apply(len)!=prop_esrf_non_empty['openalex_ids'].apply(len)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a mismatch for one of the proposals, so we investigate why this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposal</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>subject</th>\n",
       "      <th>experiment session doi</th>\n",
       "      <th>PDF document name</th>\n",
       "      <th>publications DOI</th>\n",
       "      <th>openalex_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>LS-3076</td>\n",
       "      <td>Pathogenic mechanisms of absestos-related dise...</td>\n",
       "      <td>Release of metals and dissolution of mineral f...</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>[10.15151/ESRF-ES-744175308]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10.13133/2239-1002/18090]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     proposal                                            summary  \\\n",
       "3492  LS-3076  Pathogenic mechanisms of absestos-related dise...   \n",
       "\n",
       "                                                  title        subject  \\\n",
       "3492  Release of metals and dissolution of mineral f...  Life Sciences   \n",
       "\n",
       "            experiment session doi PDF document name  \\\n",
       "3492  [10.15151/ESRF-ES-744175308]                []   \n",
       "\n",
       "                publications DOI openalex_ids  \n",
       "3492  [10.13133/2239-1002/18090]           []  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_esrf_non_empty[prop_esrf_non_empty['publications DOI'].apply(len)!=prop_esrf_non_empty['openalex_ids'].apply(len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular publication DOI (10.13133/2239-1002/18090) does not exist in the OpenAlex data repository. We shall move this proposal from prop_esrf_non_empty to prop_esrf_empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Row 3492 to prop_esrf_empty\n",
    "prop_esrf_empty=pd.concat([prop_esrf_empty,prop_esrf_non_empty.loc[3492:3493]])\n",
    "\n",
    "# Drop Row 3492 from prop_esrf_non_empty\n",
    "prop_esrf_non_empty=prop_esrf_non_empty.drop(3492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all values in the 'openalex_ids' column to an empty list\n",
    "prop_esrf_empty['openalex_ids']= np.empty((len(prop_esrf_empty), 0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the proposals appended with the OpenAlex IDs and those without\n",
    "\n",
    "prop_esrf_non_empty.to_json('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Checkpoint_data/Proposals_ESRF_openalexID', indent=2, orient='records')\n",
    "prop_esrf_empty.to_json('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Checkpoint_data/Proposals_ESRF_no_openalexID', indent=2, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
