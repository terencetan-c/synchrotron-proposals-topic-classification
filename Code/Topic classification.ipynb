{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded target vocab\n",
      "Loaded inverse target vocab\n",
      "Loaded citation features vocab.\n",
      "Loaded gold citation mapping\n",
      "Loaded gold citation L1\n",
      "Loaded non-gold citation L1\n"
     ]
    }
   ],
   "source": [
    "# Define the path\n",
    "prefix = '{insert pathname to model artifacts}'\n",
    "model_path = os.path.join(prefix, 'topic_classifier_v1_artifacts')\n",
    "\n",
    "# Load the needed files\n",
    "with open(os.path.join(model_path, \"target_vocab.pkl\"), \"rb\") as f:\n",
    "    target_vocab = pickle.load(f)\n",
    "\n",
    "print(\"Loaded target vocab\")\n",
    "\n",
    "with open(os.path.join(model_path, \"inv_target_vocab.pkl\"), \"rb\") as f:\n",
    "    inv_target_vocab = pickle.load(f)\n",
    "\n",
    "print(\"Loaded inverse target vocab\")\n",
    "\n",
    "with open(os.path.join(model_path, \"citation_feature_vocab.pkl\"), \"rb\") as f:\n",
    "    citation_feature_vocab = pickle.load(f)\n",
    "    \n",
    "print(\"Loaded citation features vocab.\")\n",
    "\n",
    "with open(os.path.join(model_path, \"gold_to_id_mapping_dict.pkl\"), \"rb\") as f:\n",
    "    gold_to_label_mapping = pickle.load(f)\n",
    "\n",
    "print(\"Loaded gold citation mapping\")\n",
    "\n",
    "with open(os.path.join(model_path, \"gold_citations_dict.pkl\"), \"rb\") as f:\n",
    "    gold_dict = pickle.load(f)\n",
    "    \n",
    "print(\"Loaded gold citation L1\")\n",
    "\n",
    "with open(os.path.join(model_path, \"non_gold_citations_dict.pkl\"), \"rb\") as f:\n",
    "    non_gold_dict = pickle.load(f)\n",
    "\n",
    "print(\"Loaded non-gold citation L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and embedding model\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "language_model_name = \\\n",
    "    \"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_keep_ind(groups):\n",
    "    \"\"\"\n",
    "    Function to determine if a text should be kept or not.\n",
    "\n",
    "    Input:\n",
    "    groups: list of character groups\n",
    "\n",
    "    Output:\n",
    "    0: if text should be not used\n",
    "    1: if text should be used\n",
    "    \"\"\"\n",
    "    # Groups of characters that do not perform well\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to remove non-latin characters.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    final_char: string of characters with non-latin characters removed\n",
    "    \"\"\"\n",
    "    final_char = []\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script not in groups_to_skip:\n",
    "                final_char.append(char)\n",
    "        except:\n",
    "            pass\n",
    "    return \"\".join(final_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to group non-latin characters and return the number of latin characters.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    groups: list of character groups\n",
    "    latin_chars: number of latin characters\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    latin_chars = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                latin_chars.append(script)\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups, len(latin_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_non_latin_characters(text):\n",
    "    \"\"\"\n",
    "    Function to check if non-latin characters are dominant in a text.\n",
    "\n",
    "    Input:\n",
    "    text: string of characters\n",
    "\n",
    "    Output:\n",
    "    0: if text should be not used\n",
    "    1: if text should be used\n",
    "    \"\"\"\n",
    "    groups, latin_chars = group_non_latin_characters(str(text))\n",
    "    if name_to_keep_ind(groups) == 1:\n",
    "        return 1\n",
    "    elif latin_chars > 20:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_emb(journal_name):\n",
    "    \"\"\"\n",
    "    Function to get journal embedding using SentenceTransformer.\n",
    "\n",
    "    Input:\n",
    "    journal_name: string of journal name\n",
    "\n",
    "    Output:\n",
    "    journal_emb: journal embedding\n",
    "    \"\"\"\n",
    "    # Strip white space\n",
    "    if isinstance(journal_name, str):\n",
    "        journal_name = journal_name.strip()\n",
    "\n",
    "        # Removing all journal names with eBook (most are not descriptive)\n",
    "        if 'eBooks' in journal_name:\n",
    "            return np.zeros(384, dtype=np.float32)\n",
    "\n",
    "        # Check if non-latin characters are dominant (embedding model not good for that)\n",
    "        elif check_for_non_latin_characters(journal_name) == 1:\n",
    "            return emb_model.encode(journal_name)\n",
    "\n",
    "        elif journal_name == '':\n",
    "            return np.zeros(384, dtype=np.float32)\n",
    "\n",
    "        else:\n",
    "            return np.zeros(384, dtype=np.float32)\n",
    "    else:\n",
    "        return np.zeros(384, dtype=np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to tokenize text using model tokenizer.\n",
    "    \n",
    "    Input:\n",
    "    seq: string of text\n",
    "    \n",
    "    Output:\n",
    "    tok_data: dictionary of tokenized text\n",
    "    \"\"\"\n",
    "    tok_data = tokenizer(seq, max_length=512, truncation=True, padding='max_length', **kwargs)\n",
    "    return [tok_data['input_ids'], tok_data['attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_level_0_to_1(level_0, level_1):\n",
    "    \"\"\"\n",
    "    Function to move level 0 citations to level 1 citations.\n",
    "    \n",
    "    Input:\n",
    "    level_0: list of level 0 citations\n",
    "    level_1: list of level 1 citations\n",
    "    \n",
    "    Output:\n",
    "    list of final level 1 citations\"\"\"\n",
    "    return list(set(level_0 + level_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_citations_for_model(list_of_links, num_to_take):\n",
    "    \"\"\"\n",
    "    Function to get final citations for model if there are more than num_to_take citations.\n",
    "    \n",
    "    Input:\n",
    "    list_of_links: list of citations\n",
    "    num_to_take: number of citations to take\n",
    "    \n",
    "    Output:\n",
    "    list of final citations\n",
    "    \"\"\"\n",
    "    if len(list_of_links) <= num_to_take:\n",
    "        return list_of_links\n",
    "    else:\n",
    "        return random.sample(list_of_links, num_to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_citations_feature(citations, num_to_keep):\n",
    "    \"\"\"\n",
    "    Function to get final citations for model if there are more than num_to_take citations\n",
    "    and also to map the citations to gold citation ids.\n",
    "\n",
    "    Input:\n",
    "    citations: list of citations\n",
    "    num_to_keep: number of citations to take\n",
    "\n",
    "    Output:\n",
    "    list of final citations\n",
    "    \"\"\"\n",
    "    if citations:\n",
    "        new_citations = get_final_citations_for_model(citations, num_to_keep)\n",
    "        mapped_cites = [gold_to_label_mapping.get(x) for x in new_citations \n",
    "                        if gold_to_label_mapping.get(x)]\n",
    "        temp_feature = [citation_feature_vocab[x] for x in mapped_cites]\n",
    "    \n",
    "        if len(temp_feature) < num_to_keep:\n",
    "            return temp_feature + [0]*(num_to_keep - len(temp_feature))\n",
    "        else:\n",
    "            return temp_feature\n",
    "    else:\n",
    "        return [1] + [0]*(num_to_keep - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_abstract(title, abstract):\n",
    "    \"\"\"\n",
    "    Function to merge title and abstract together for model input.\n",
    "    \n",
    "    Input:\n",
    "    title: string of title\n",
    "    abstract: string of abstract\n",
    "    \n",
    "    Output:\n",
    "    string of title and abstract merged together\"\"\"\n",
    "    if isinstance(title, str):\n",
    "        if isinstance(abstract, str):\n",
    "            if len(abstract) >=30:\n",
    "                return f\"<TITLE> {title}\\n<ABSTRACT> {abstract[:2500]}\"\n",
    "            else:\n",
    "                return f\"<TITLE> {title}\"\n",
    "        else:\n",
    "            return f\"<TITLE> {title}\"\n",
    "    else:\n",
    "        if isinstance(abstract, str):\n",
    "            if len(abstract) >=30:\n",
    "                return f\"<TITLE> NONE\\n<ABSTRACT> {abstract[:2500]}\"\n",
    "            else:\n",
    "                return \"\"\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(old_title):\n",
    "    \"\"\"\n",
    "    Function to check if title should be kept and then remove non-latin characters. Also\n",
    "    removes some HTML tags from the title.\n",
    "    \n",
    "    Input:\n",
    "    old_title: string of title\n",
    "    \n",
    "    Output:\n",
    "    new_title: string of title with non-latin characters and HTML tags removed\n",
    "    \"\"\"\n",
    "    keep_title = check_for_non_latin_characters(old_title)\n",
    "    if keep_title == 1:\n",
    "        new_title = remove_non_latin_characters(old_title)\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\"<i>\", \"\").replace(\"</i>\",\"\")\\\n",
    "                                 .replace(\"<sub>\", \"\").replace(\"</sub>\",\"\") \\\n",
    "                                 .replace(\"<sup>\", \"\").replace(\"</sup>\",\"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\",\"\") \\\n",
    "                                 .replace(\"<b>\", \"\").replace(\"</b>\",\"\") \\\n",
    "                                 .replace(\"<I>\", \"\").replace(\"</I>\", \"\") \\\n",
    "                                 .replace(\"<SUB>\", \"\").replace(\"</SUB>\", \"\") \\\n",
    "                                 .replace(\"<scp>\", \"\").replace(\"</scp>\", \"\") \\\n",
    "                                 .replace(\"<font>\", \"\").replace(\"</font>\", \"\") \\\n",
    "                                 .replace(\"<inf>\",\"\").replace(\"</inf>\", \"\") \\\n",
    "                                 .replace(\"<i /> \", \"\") \\\n",
    "                                 .replace(\"<p>\", \"\").replace(\"</p>\",\"\") \\\n",
    "                                 .replace(\"<![CDATA[<B>\", \"\").replace(\"</B>]]>\", \"\") \\\n",
    "                                 .replace(\"<italic>\", \"\").replace(\"</italic>\",\"\")\\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<br>\", \"\").replace(\"</br>\",\"\").replace(\"<br/>\",\"\") \\\n",
    "                                 .replace(\"<B>\", \"\").replace(\"</B>\", \"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\", \"\") \\\n",
    "                                 .replace(\"<BR>\", \"\").replace(\"</BR>\", \"\") \\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<strong>\", \"\").replace(\"</strong>\", \"\") \\\n",
    "                                 .replace(\"<formula>\", \"\").replace(\"</formula>\", \"\") \\\n",
    "                                 .replace(\"<roman>\", \"\").replace(\"</roman>\", \"\") \\\n",
    "                                 .replace(\"<SUP>\", \"\").replace(\"</SUP>\", \"\") \\\n",
    "                                 .replace(\"<SSUP>\", \"\").replace(\"</SSUP>\", \"\") \\\n",
    "                                 .replace(\"<sc>\", \"\").replace(\"</sc>\", \"\") \\\n",
    "                                 .replace(\"<subtitle>\", \"\").replace(\"</subtitle>\", \"\") \\\n",
    "                                 .replace(\"<emph/>\", \"\").replace(\"<emph>\", \"\").replace(\"</emph>\", \"\") \\\n",
    "                                 .replace(\"\"\"<p class=\"Body\">\"\"\", \"\") \\\n",
    "                                 .replace(\"<TITLE>\", \"\").replace(\"</TITLE>\", \"\") \\\n",
    "                                 .replace(\"<sub />\", \"\").replace(\"<sub/>\", \"\") \\\n",
    "                                 .replace(\"<mi>\", \"\").replace(\"</mi>\", \"\") \\\n",
    "                                 .replace(\"<bold>\", \"\").replace(\"</bold>\", \"\") \\\n",
    "                                 .replace(\"<mtext>\", \"\").replace(\"</mtext>\", \"\") \\\n",
    "                                 .replace(\"<msub>\", \"\").replace(\"</msub>\", \"\") \\\n",
    "                                 .replace(\"<mrow>\", \"\").replace(\"</mrow>\", \"\") \\\n",
    "                                 .replace(\"</mfenced>\", \"\").replace(\"</math>\", \"\")\n",
    "\n",
    "            if '<mml' in new_title:\n",
    "                all_parts = [x for y in [i.split(\"mml:math>\") for i in new_title.split(\"<mml:math\")] for x in y if x]\n",
    "                final_parts = []\n",
    "                for part in all_parts:\n",
    "                    if re.search(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part):\n",
    "                        pull_out = re.findall(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part)\n",
    "                        final_pieces = []\n",
    "                        for piece in pull_out:\n",
    "                            final_pieces.append(piece.replace(\">\", \"\").replace(\"<\", \"\"))\n",
    "                        \n",
    "                        final_parts.append(\" \"+ \"\".join(final_pieces) + \" \")\n",
    "                    else:\n",
    "                        final_parts.append(part)\n",
    "                \n",
    "                new_title = \"\".join(final_parts).strip()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if '<xref' in new_title:\n",
    "                new_title = re.sub(r\"\\<xref[^/]*\\/xref\\>\", \"\", new_title)\n",
    "\n",
    "            if '<inline-formula' in new_title:\n",
    "                new_title = re.sub(r\"\\<inline-formula[^/]*\\/inline-formula\\>\", \"\", new_title)\n",
    "\n",
    "            if '<title' in new_title:\n",
    "                new_title = re.sub(r\"\\<title[^/]*\\/title\\>\", \"\", new_title)\n",
    "\n",
    "            if '<p class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<p class=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if '<span class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<span class=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "            if 'mfenced open' in new_title:\n",
    "                new_title = re.sub(r\"\\<mfenced open=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if 'math xmlns' in new_title:\n",
    "                new_title = re.sub(r\"\\<math xmlns=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\">i<\", \"\").replace(\">/i<\", \"\") \\\n",
    "                                 .replace(\">b<\", \"\").replace(\">/b<\", \"\") \\\n",
    "                                 .replace(\"<inline-formula>\", \"\").replace(\"</inline-formula>\",\"\")\n",
    "\n",
    "        return new_title\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_abstract(raw_abstract, inverted=False):\n",
    "    \"\"\"\n",
    "    Function to clean abstract and return it in a format for the model.\n",
    "    \n",
    "    Input:\n",
    "    raw_abstract: string of abstract\n",
    "    inverted: boolean to determine if abstract is inverted index or not\n",
    "    \n",
    "    Output:\n",
    "    final_abstract: string of abstract in format for model\n",
    "    \"\"\"\n",
    "    if inverted:\n",
    "        if isinstance(raw_abstract, dict) | isinstance(raw_abstract, str):\n",
    "            if isinstance(raw_abstract, dict):\n",
    "                invert_abstract = raw_abstract\n",
    "            else:\n",
    "                invert_abstract = json.loads(raw_abstract)\n",
    "            \n",
    "            if invert_abstract.get('IndexLength'):\n",
    "                ab_len = invert_abstract['IndexLength']\n",
    "\n",
    "                if ab_len > 20:\n",
    "                    abstract = [\" \"]*ab_len\n",
    "                    for key, value in invert_abstract['InvertedIndex'].items():\n",
    "                        for i in value:\n",
    "                            abstract[i] = key\n",
    "                    final_abstract = \" \".join(abstract)[:2500]\n",
    "                    keep_abs = check_for_non_latin_characters(final_abstract)\n",
    "                    if keep_abs == 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        final_abstract = None\n",
    "                else:\n",
    "                    final_abstract = None\n",
    "            else:\n",
    "                if len(invert_abstract) > 20:\n",
    "                    abstract = [\" \"]*1200\n",
    "                    for key, value in invert_abstract.items():\n",
    "                        for i in value:\n",
    "                            try:\n",
    "                                abstract[i] = key\n",
    "                            except:\n",
    "                                pass\n",
    "                    final_abstract = \" \".join(abstract)[:2500].strip()\n",
    "                    keep_abs = check_for_non_latin_characters(final_abstract)\n",
    "                    if keep_abs == 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        final_abstract = None\n",
    "                else:\n",
    "                    final_abstract = None\n",
    "                \n",
    "        else:\n",
    "            final_abstract = None\n",
    "    else:\n",
    "        ab_len = len(raw_abstract)\n",
    "        if ab_len > 30:\n",
    "            final_abstract = raw_abstract[:2500]\n",
    "            keep_abs = check_for_non_latin_characters(final_abstract)\n",
    "            if keep_abs == 1:\n",
    "                pass\n",
    "            else:\n",
    "                final_abstract = None\n",
    "        else:\n",
    "            final_abstract = None\n",
    "            \n",
    "    return final_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_feature(features):\n",
    "    \"\"\"\n",
    "    Function to create input feature for model.\n",
    "    \n",
    "    Input:\n",
    "    features: list of features\n",
    "    \n",
    "    Output:\n",
    "    input_feature: list of features in format for model\"\"\"\n",
    "    # Convert to a tensorflow feature\n",
    "    input_feature = [tf.expand_dims(tf.convert_to_tensor(x), axis=0) for x in [np.array(features[0], dtype=np.int32), \n",
    "                                                                             np.array(features[1], dtype=np.int32), \n",
    "                                                                             features[2]]]\n",
    "\n",
    "    return input_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_citations_from_all_citations(all_citations, gold_dict, non_gold_dict):\n",
    "    \"\"\"\n",
    "    Function to get gold citations from all citations.\n",
    "    \n",
    "    Input:\n",
    "    all_citations: list of all citations\n",
    "    gold_dict: dictionary of gold citations\n",
    "    non_gold_dict: dictionary of non-gold citations\n",
    "    \n",
    "    Output:\n",
    "    level_0_gold: list of level 0 gold citations\n",
    "    level_1_gold: list of level 1 gold citations\n",
    "    \"\"\"\n",
    "    if isinstance(all_citations, list):\n",
    "        if len(all_citations) > 200:\n",
    "            all_citations = random.sample(all_citations, 200)\n",
    "        \n",
    "        level_0_gold_temp = [[x, gold_dict.get(x)] for x in all_citations if gold_dict.get(x)]\n",
    "\n",
    "        level_1_gold_temp = [non_gold_dict.get(x) for x in all_citations if non_gold_dict.get(x)]\n",
    "\n",
    "        level_0_gold = [x[0] for x in level_0_gold_temp]\n",
    "        level_1_gold = [y for z in [x[1] for x in level_0_gold_temp] for y in z] + \\\n",
    "                        [x for y in level_1_gold_temp for x in y]\n",
    "\n",
    "        return level_0_gold, level_1_gold\n",
    "    else:\n",
    "        return [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, emb_table_size, model_chkpt, topk=5):\n",
    "    \"\"\"\n",
    "    Function to create full model.\n",
    "    \n",
    "    Input:\n",
    "    num_classes: number of classes\n",
    "    emb_table_size: size of embedding table\n",
    "    model_chkpt: path to model checkpoint\n",
    "    topk: number of predictions to return\n",
    "    \n",
    "    Output:\n",
    "    model: full model\n",
    "    \"\"\"\n",
    "    # Inputs\n",
    "    citation_0 = tf.keras.layers.Input((16,), dtype=tf.int64, name='citation_0')\n",
    "    citation_1 = tf.keras.layers.Input((128,), dtype=tf.int64, name='citation_1')\n",
    "    journal = tf.keras.layers.Input((384,), dtype=tf.float32, name='journal_emb')\n",
    "    language_model_output = tf.keras.layers.Input((512, 768,), dtype=tf.float32, name='lang_model_output')\n",
    "    \n",
    "    # Create a multi-class classification model using functional API\n",
    "    pooled_language_model_output = tf.keras.layers.GlobalAveragePooling1D()(language_model_output)\n",
    "    citation_emb_layer = tf.keras.layers.Embedding(input_dim=emb_table_size, output_dim=256, mask_zero=True, \n",
    "                                                   trainable=True, name='citation_emb_layer')\n",
    "\n",
    "    citation_0_emb = citation_emb_layer(citation_0)\n",
    "    citation_1_emb = citation_emb_layer(citation_1)\n",
    "\n",
    "    pooled_citation_0 = tf.keras.layers.GlobalAveragePooling1D()(citation_0_emb)\n",
    "    pooled_citation_1 = tf.keras.layers.GlobalAveragePooling1D()(citation_1_emb)\n",
    "\n",
    "    concat_data = tf.keras.layers.Concatenate(name='concat_data', axis=-1)([pooled_language_model_output, pooled_citation_0, \n",
    "                                                                            pooled_citation_1, journal])\n",
    "\n",
    "    # Dense layer 1\n",
    "    dense_output = tf.keras.layers.Dense(2048, activation='relu', kernel_regularizer='L2', name=\"dense_1\")(concat_data)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_1\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")(dense_output)\n",
    "    \n",
    "    # Dense layer 2\n",
    "    dense_output = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='L2', name=\"dense_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")(dense_output)\n",
    "\n",
    "    # Dense layer 3\n",
    "    dense_output_l3 = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer='L2', name=\"dense_3\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_3\")(dense_output_l3)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_3\")(dense_output)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='output_layer')(dense_output)\n",
    "    topk_outputs = tf.math.top_k(output_layer, k=topk)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[citation_0, citation_1, journal, language_model_output], \n",
    "                           outputs=topk_outputs)\n",
    "\n",
    "    model.load_weights(model_chkpt)\n",
    "    model.trainable = False\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_ids_and_scores_bad(topic_ids, score, labels, title, abstract, threshold=0.04):\n",
    "    \"\"\"\n",
    "    Function to apply some rules to get the final prediction (some clusters performed worse than others).\n",
    "    \n",
    "    Input:\n",
    "    topic_ids: all ids for raw prediction output\n",
    "    score: all scores for raw prediction output\n",
    "    labels: all labels for raw prediction output\n",
    "    title: title of the work\n",
    "    abstract: abstract of the work\n",
    "    \n",
    "    Output:\n",
    "    final_ids: post-processed final ids\n",
    "    final_scores: post-processed final scores\n",
    "    final_labels: post-processed final labels\n",
    "    \"\"\"\n",
    "    final_ids = [-1]\n",
    "    final_scores = [0.0]\n",
    "    final_labels = [None]\n",
    "    if any(topic_id in topic_ids for topic_id in [13241]):\n",
    "        return final_ids, final_scores, final_labels\n",
    "    elif any(topic_id in topic_ids for topic_id in [12705,13003]):\n",
    "        if title != '':\n",
    "            if check_for_non_latin_characters(title) == 1:\n",
    "                if len(title.split(\" \")) > 9:\n",
    "                    if not isinstance(abstract, str):\n",
    "                        final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                        final_scores = [y for y in score if y > threshold]\n",
    "                        final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "                        if final_ids:\n",
    "                            return final_ids, final_scores, final_labels\n",
    "                        else:\n",
    "                            return [-1], [0.0], [None]\n",
    "                    elif isinstance(abstract, str):\n",
    "                        if check_for_non_latin_characters(abstract) == 1:\n",
    "                            final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                            final_scores = [y for y in score if y > 0.05]\n",
    "                            final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "                            if final_ids:\n",
    "                                return final_ids, final_scores, final_labels\n",
    "                            else:\n",
    "                                return [-1], [0.0], [None]\n",
    "                        else:\n",
    "                            return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "            else:\n",
    "                return final_ids, final_scores, final_labels\n",
    "        else:\n",
    "            return final_ids, final_scores, final_labels\n",
    "    else:\n",
    "        if any(topic_id in topic_ids for topic_id in [12718,14377,13686,13723]):\n",
    "            final_ids = [x for x,y in zip(topic_ids, score) if (x not in [12718,14377,13686,13723]) & (y > 0.80)]\n",
    "            final_scores = [y for x,y in zip(topic_ids, score) if (x not in [12718,14377,13686,13723]) & (y > 0.80)]\n",
    "            final_labels = [y for x,y,z in zip(topic_ids, labels, score) if (x not in [12718,14377,13686,13723]) & (z > 0.80)]\n",
    "            if final_ids:\n",
    "                return final_ids, final_scores, final_labels\n",
    "            else:\n",
    "                return [-1], [0.0], [None]\n",
    "        elif any(topic_id in topic_ids for topic_id in [13064, 13537]):\n",
    "            if title == 'Frontmatter':\n",
    "                return [-1], [0.0], [None]\n",
    "            else:\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if (((x in [13064, 13537]) & (y > 0.95)) | \n",
    "                                                                ((x not in [13064, 13537]) & (y > threshold)))]\n",
    "                final_scores = [y for x,y in zip(topic_ids, score) if (((x in [13064, 13537]) & (y > 0.95)) | \n",
    "                                                                    ((x not in [13064, 13537]) & (y > threshold)))]\n",
    "                final_labels = [z for x,y,z in zip(topic_ids, score, labels) if (((x in [13064, 13537]) & (y > 0.95)) | \n",
    "                                                                    ((x not in [13064, 13537]) & (y > threshold)))]\n",
    "                if final_ids:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "        elif any(topic_id in topic_ids for topic_id in [11893, 13459]):\n",
    "            test_scores = [y for x,y in zip(topic_ids, score) if (x in [11893, 13459])]\n",
    "            if topic_ids[0] in [11893, 13459]:\n",
    "                first_pred = 1\n",
    "            else:\n",
    "                first_pred = 0\n",
    "            \n",
    "            if [x for x in test_scores if x > 0.95] & (first_pred == 1):\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                final_scores = [y for y in score if y > 0.05]\n",
    "                final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "\n",
    "                if final_ids:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            elif first_pred == 0:\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                final_scores = [y for y in score if y > threshold]\n",
    "                final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "\n",
    "                if final_ids:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            else:\n",
    "                return [-1], [0.0], [None]\n",
    "        else:\n",
    "            if isinstance(abstract, str) & (title != ''):\n",
    "                if (check_for_non_latin_characters(title) == 1) & (check_for_non_latin_characters(abstract) == 1):\n",
    "                    final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                    final_scores = [y for y in score if y > threshold]\n",
    "                    final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "    \n",
    "                    if final_ids:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return [-1], [0.0], [None]\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            elif title != '':\n",
    "                if (check_for_non_latin_characters(title) == 1):\n",
    "                    final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                    final_scores = [y for y in score if y > threshold]\n",
    "                    final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "    \n",
    "                    if final_ids:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return [-1], [0.0], [None]\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            elif isinstance(abstract, str):\n",
    "                if (check_for_non_latin_characters(abstract) == 1):\n",
    "                    final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                    final_scores = [y for y in score if y > threshold]\n",
    "                    final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "    \n",
    "                    if final_ids:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return [-1], [0.0], [None]\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            else:\n",
    "                return [-1], [0.0], [None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_as_df(new_df):\n",
    "    \"\"\"\n",
    "    Function to process data as a dataframe (in batch).\n",
    "    \n",
    "    Input:\n",
    "    new_df: dataframe of data\n",
    "    \n",
    "    Output:\n",
    "    input_df: dataframe of data with predictions\n",
    "    \"\"\"\n",
    "    input_df = new_df.copy()\n",
    "    # Get citations into integer format\n",
    "    input_df['referenced_works'] = input_df['referenced_works'].apply(lambda x: [int(i.split(\"https://openalex.org/W\")[1]) for \n",
    "                                                                             i in x])\n",
    "     # Process title and abstract and tokenize\n",
    "    input_df['title'] = input_df['title'].apply(lambda x: clean_title(x))\n",
    "    input_df['abstract_inverted_index'] = input_df.apply(lambda x: clean_abstract(x.abstract_inverted_index, x.inverted), axis=1)\n",
    "    title_abstract = input_df.apply(lambda x: merge_title_and_abstract(x.title, x.abstract_inverted_index), axis=1).tolist()\n",
    "    tok_inputs_pt = tokenize(title_abstract, return_tensors='pt')       # Tokenise merged title and abstract using tokensier from the OpenAlex BERT model, tokens are returned as a PyTorch tensor\n",
    "    with torch.no_grad():       # Tells PyTorch not to compute gradients (i.e. it is in inference mode) since we are not training the model but just using it to get outputs, saving memory and computation\n",
    "        last_output = pt_model(*tok_inputs_pt).hidden_states[-1]        # Get hidden states from last layer of the OpenAlex BERT model, which often holds the most refined understanding of the input\n",
    "    lang_model_output = last_output.numpy()     # Convert last_output from PyTorch tensor to NumPy array\n",
    "    \n",
    "    # Take citations and return only gold citations (and then convert to label ids)\n",
    "    input_df['referenced_works'] = input_df['referenced_works'].apply(lambda x: get_gold_citations_from_all_citations(x, gold_dict, \n",
    "                                                                                                                      non_gold_dict))\n",
    "    input_df['citation_0'] = input_df['referenced_works'].apply(lambda x: get_final_citations_feature(x[0], 16))\n",
    "    input_df['citation_1'] = input_df['referenced_works'].apply(lambda x: get_final_citations_feature(x[1], 128))    \n",
    "    \n",
    "    # Take in journal name and output journal embedding\n",
    "    input_df['journal_emb'] = input_df['journal_display_name'].apply(get_journal_emb)\n",
    "\n",
    "    # Check completeness of input data\n",
    "    input_df['score_data'] = input_df\\\n",
    "        .apply(lambda x: 0 if ((x.title == \"\") & \n",
    "                               (not x.abstract_inverted_index) & \n",
    "                               (x.citation_0[0]==1) & \n",
    "                               (x.citation_1[0]==1)) else 1, axis=1)\n",
    "\n",
    "    data_to_score = input_df[input_df['score_data']==1].copy()\n",
    "    data_to_not_score = input_df[input_df['score_data']==0][['UID']].copy()\n",
    "\n",
    "    if data_to_score.shape[0] > 0:\n",
    "        # Transform into output for model\n",
    "        data_to_score['input_feature'] = data_to_score.apply(lambda x: create_input_feature([x.citation_0, x.citation_1, \n",
    "                                                                                             x.journal_emb]), axis=1)\n",
    "    \n",
    "        all_rows = [tf.convert_to_tensor([x[0][0] for x in data_to_score['input_feature'].tolist()]), \n",
    "                    tf.convert_to_tensor([x[1][0] for x in data_to_score['input_feature'].tolist()]), \n",
    "                    tf.convert_to_tensor([x[2][0] for x in data_to_score['input_feature'].tolist()]), \n",
    "                    tf.convert_to_tensor(lang_model_output)]\n",
    "        \n",
    "        preds = xla_predict(all_rows)\n",
    "        \n",
    "        data_to_score['preds'] = preds.indices.numpy().tolist()\n",
    "        data_to_score['scores'] = preds.values.numpy().tolist()\n",
    "    else:\n",
    "        data_to_score['preds'] = [[-1]]*data_to_not_score.shape[0]\n",
    "        data_to_score['scores'] = [[0.0000]]*data_to_not_score.shape[0]\n",
    "    \n",
    "    data_to_not_score['preds'] = [[-1]]*data_to_not_score.shape[0]\n",
    "    data_to_not_score['scores'] = [[0.0000]]*data_to_not_score.shape[0]\n",
    "    \n",
    "    return input_df[['UID','title','abstract_inverted_index']].merge(pd.concat([data_to_score[['UID','preds','scores']], \n",
    "                                              data_to_not_score[['UID','preds','scores']]], axis=0), \n",
    "                                   how='left', on='UID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_pred_check(old_preds, old_scores, old_labels):\n",
    "    \"\"\"\n",
    "    Function to apply some rules to get the final prediction based on scores\n",
    "    \n",
    "    Input:\n",
    "    old_preds: all ids for prediction output\n",
    "    old_scores: all scores for prediction output\n",
    "    old_labels: all labels for prediction output\n",
    "    \n",
    "    Output:\n",
    "    final_ids: post-processed final ids\n",
    "    final_scores: post-processed final scores\n",
    "    final_labels: post-processed final labels\n",
    "    \"\"\"\n",
    "    pred_scores = [[x,y,z] for x,y,z in zip(old_preds, old_scores, old_labels)]\n",
    "\n",
    "    # if any of scores are over 0.9\n",
    "    if [x[1] for x in pred_scores if x[1] > 0.9]:\n",
    "        final_pred_scores = [[x[0], x[1], x[2]] for x in pred_scores if x[1] > 0.9]\n",
    "    elif len(pred_scores) == 1:\n",
    "        final_pred_scores = pred_scores.copy()\n",
    "    elif len(pred_scores) == 2:\n",
    "        scores = [x[1] for x in pred_scores]\n",
    "        if scores[1] < (scores[0]/2):\n",
    "            final_pred_scores = pred_scores[:1].copy()\n",
    "        else:\n",
    "            final_pred_scores = pred_scores.copy()\n",
    "    else:\n",
    "        preds = [x[0] for x in pred_scores]\n",
    "        scores = [x[1] for x in pred_scores]\n",
    "        labels = [x[2] for x in pred_scores]\n",
    "\n",
    "        score_sum = scores[0]\n",
    "        final_pred_scores = pred_scores[:1].copy()\n",
    "        for i, (pred, score, label) in enumerate(zip(preds[1:], scores[1:], labels[1:])):\n",
    "            if score < (score_sum/(i+1)*0.85):\n",
    "                break\n",
    "            else:\n",
    "                final_pred_scores.append([pred, score, label])\n",
    "                score_sum += score\n",
    "\n",
    "    final_preds = [x[0] for x in final_pred_scores]\n",
    "    final_scores = [x[1] for x in final_pred_scores]\n",
    "    final_labels = [x[2] for x in final_pred_scores]\n",
    "    return final_preds, final_scores, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n"
     ]
    }
   ],
   "source": [
    "# Loading the models\n",
    "pred_model = create_model(len(target_vocab), \n",
    "                          len(citation_feature_vocab)+2,\n",
    "                          os.path.join(model_path, \"model_checkpoint/citation_part_only.keras\"), topk=3)\n",
    "xla_predict = tf.function(pred_model, jit_compile=True)\n",
    "\n",
    "\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(language_model_name, output_hidden_states=True)\n",
    "pt_model.eval()\n",
    "\n",
    "print(\"Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a prediction for the model.\n",
    "\n",
    "Input:\n",
    "JSON of data\n",
    "\n",
    "Output:\n",
    "JSON of predictions\n",
    "\"\"\"\n",
    "# Get input JSON data and convert it to a DF\n",
    "def full_model_prediction(data):\n",
    "    input_json = data\n",
    "    if isinstance(input_json, list):\n",
    "        pass\n",
    "    else:\n",
    "        input_json = json.loads(input_json)\n",
    "\n",
    "    input_df = pd.DataFrame.from_dict(input_json).reset_index().rename(columns={'index': 'UID'})\n",
    "\n",
    "    final_preds = process_data_as_df(input_df)\n",
    "    all_tags = []\n",
    "    threshold = 0.04\n",
    "    for pred,score,title,abstract in zip(final_preds['preds'].tolist(), final_preds['scores'].tolist(), \n",
    "                                final_preds['title'].tolist(), final_preds['abstract_inverted_index'].tolist()):\n",
    "        if pred[0] == -1:\n",
    "            final_ids = [-1]\n",
    "            final_scores = [0.0]\n",
    "            final_labels = [None]\n",
    "        else:\n",
    "            topic_labels = [inv_target_vocab[i] for i in pred]\n",
    "            topic_ids = [int(i.split(': ')[0]) + 10000 for i in topic_labels]\n",
    "            \n",
    "            if any(topic_id in topic_ids for topic_id in [13241,12705,13003,12718,14377,13686,13723,13064, 13537,11893, 13459,13444]):\n",
    "                final_ids, final_scores, final_labels = get_final_ids_and_scores_bad(topic_ids, score, topic_labels, title, abstract)\n",
    "            else:\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                final_scores = [y for y in score if y > threshold]\n",
    "                final_labels = [x for x,y in zip(topic_labels, score) if y > threshold]\n",
    "\n",
    "        if final_ids:\n",
    "            if final_ids[0] != -1:\n",
    "                final_ids, final_scores, final_labels = last_pred_check(final_ids, final_scores, final_labels)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            final_ids = [-1]\n",
    "            final_scores = [0.0]\n",
    "            final_labels = [None]\n",
    "\n",
    "        single_tags = []\n",
    "        _ = [single_tags.append({'topic_id': i,\n",
    "                                    'topic_label': k, \n",
    "                                    'topic_score': round(float(j), 4)}) if i != -1 else \n",
    "                single_tags.append({'topic_id': -1,\n",
    "                                    'topic_label': None, \n",
    "                                    'topic_score': round(0.0, 4)}) for i,j,k in zip(final_ids, final_scores, final_labels)]\n",
    "        all_tags.append(single_tags)\n",
    "\n",
    "    # Transform predictions to JSON\n",
    "    result = json.dumps(all_tags)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialised the model, we import the proposal datafiles we got from the 'Data Processing' Jupyter notebook.\\\n",
    "We have to transform the proposal DataFrames to the specified format for input into the model.\\\n",
    "For proposals with no referenced works, we will set the input features as an empty list.\\\n",
    "Note that we will set the journal name to a blank string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_esrf_non_empty=pd.read_json('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Proposals_ESRF_openalexID')\n",
    "prop_esrf_empty=pd.read_json('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Proposals_ESRF_no_openalexID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove some proposals with no publications DOIs (missed them earlier)\n",
    "\n",
    "# prop_esrf_non_empty=prop_esrf_non_empty[prop_esrf_non_empty['openalex_ids'].apply(lambda x: x != [[]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names\n",
    "\n",
    "prop_esrf_non_empty=prop_esrf_non_empty.rename(columns={'summary':'abstract_inverted_index','openalex_ids':'referenced_works'})\n",
    "prop_esrf_empty=prop_esrf_empty.rename(columns={'summary':'abstract_inverted_index','openalex_ids':'referenced_works'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all null values in the title and abstract columns with blank string\n",
    "\n",
    "prop_esrf_non_empty['abstract_inverted_index'].fillna('',inplace=True)\n",
    "prop_esrf_non_empty['title'].fillna('',inplace=True)\n",
    "\n",
    "prop_esrf_empty['abstract_inverted_index'].fillna('',inplace=True)\n",
    "prop_esrf_empty['title'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training batch\n",
    "\n",
    "batch_non_empty=prop_esrf_non_empty[['title','abstract_inverted_index','referenced_works']].to_dict(orient='records')\n",
    "batch_empty=prop_esrf_empty[['title','abstract_inverted_index','referenced_works']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input features\n",
    "\n",
    "for dict in batch_non_empty:\n",
    "    dict['journal_display_name']=''\n",
    "    dict['inverted']=False\n",
    "\n",
    "for dict in batch_empty:\n",
    "    dict['journal_display_name']=''\n",
    "    dict['inverted']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_non_empty[687]['referenced_works']=['https://openalex.org/W3115800845',\n",
    "#   'https://openalex.org/W3120781410']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have formatted the input data correctly, we can proceed with topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-11-15 16:03:26.918207: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x543de0130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-15 16:03:26.918341: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-11-15 16:03:26.972243: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-15 16:03:27.231386: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Proposals with referenced works\n",
    "results_non_empty=[]\n",
    "for single in batch_non_empty:\n",
    "    result=full_model_prediction([single])\n",
    "    results_non_empty.append(json.loads(result)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposals with no referenced works\n",
    "results_empty=[]\n",
    "for single in batch_empty:\n",
    "    result=full_model_prediction([single])\n",
    "    results_empty.append(json.loads(result)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the list of topic predictions as a dict value to the batch lists\n",
    "\n",
    "n = len(batch_non_empty)\n",
    "for i in range(n):\n",
    "    batch_non_empty[i]['topic predictions']=results_non_empty[i]\n",
    "    batch_non_empty[i]['has referenced works']=1\n",
    "\n",
    "\n",
    "n = len(batch_empty)\n",
    "for i in range(n):\n",
    "    batch_empty[i]['topic predictions']=results_empty[i]\n",
    "    batch_empty[i]['has referenced works']=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both non_empty and empty batches\n",
    "prop_esrf_pred=batch_non_empty + batch_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to json string\n",
    "json_string=json.dumps(prop_esrf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the JSON string to a file\n",
    "with open('{insert pathname here}/Datasets/ESRF/Proposals_ESRF_Predictions', 'w') as file:\n",
    "    file.write(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
