{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing publications\n",
    "\n",
    "The publications from each facility aare imported.\\\n",
    "The publications were downloaded from the databases of the various facilities.\\\n",
    "For HZB, an API call is used to download the publications\\\n",
    "\n",
    "ESRF/ILL: https://epn-library.esrf.fr/ \\\n",
    "DLS: https://publications.diamond.ac.uk/ \\\n",
    "HZB: https://www.helmholtz-berlin.de/pubbin/publikationen.pl \\\n",
    "\n",
    "Only the ESRF section of the code is complete; the rest are work in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of ESRF publications. We shall designate them as follows:\n",
    "<br>\n",
    "Type 1: Publications with ESRF authors and describing ESRF experiments\n",
    "<br>\n",
    "Type 2: Publications without ESRF authors and describing ESRF experiments\n",
    "<br>\n",
    "Type 3: Publications with ESRF authors and not describing ESRF experiments\n",
    "<br>\n",
    "Type 4: Articles citing the ESRF, no ESRF authors\n",
    "<br>\n",
    "This 'Type' information willl be included alongside the other metadata in the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all ESRF publication files\n",
    "pub_esrf_type1=pd.read_csv('{insert path here}/Datasets/ESRF/ESRF publications with ESRF authors and describing ESRF experiment (Oct 2024).csv',sep='\t', encoding='utf-8',skiprows=1,header=0)    # Import file\n",
    "pub_esrf_type2=pd.concat([pd.read_csv('{insert path here}/Datasets/ESRF/ESRF Publications without ESRF authors and describing an ESRF experiment (2015 onwards).csv',sep='\t', encoding='utf-8',skiprows=1,header=0),pd.read_csv('Datasets/ESRF/ESRF Publications without ESRF authors and describing an ESRF experiment (bef 2015).csv',sep='\t', encoding='utf-8',skiprows=1,header=0)],ignore_index=True)    # Import file\n",
    "pub_esrf_type3=pd.read_csv('{insert path here}/Datasets/ESRF/ESRF Publications with ESRF authors and not describing an ESRF experiment.csv',sep='\t', encoding='utf-8',skiprows=1,header=0)    # Import file\n",
    "pub_esrf_type4=pd.read_csv('{insert path here}/Datasets/ESRF/ESRF articles citing ESRF, no ESRF author.csv',sep='\t', encoding='utf-8',skiprows=1,header=0)    # Import file\n",
    "\n",
    "# Add 'Type' column\n",
    "pub_esrf_type1['Type']=1\n",
    "pub_esrf_type2['Type']=2\n",
    "pub_esrf_type3['Type']=3\n",
    "pub_esrf_type4['Type']=4\n",
    "\n",
    "# Concatenate all four types\n",
    "pub_esrf=pd.concat([pub_esrf_type1,pub_esrf_type2,pub_esrf_type3,pub_esrf_type4],ignore_index=True)\n",
    "\n",
    "# Add 'Facility repository' column\n",
    "pub_esrf['Facility repository']='ESRF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to csv file\n",
    "pub_esrf.to_csv('{insert path here}/Publications_ESRF',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of ILL publications. We shall designate them as follows:\n",
    "<br>\n",
    "Type 1: Publications with ILL authors and describing ILL experiments\n",
    "<br>\n",
    "Type 2: Publications without ILL authors and describing ILL experiments\n",
    "<br>\n",
    "Type 3: Publications with ILL authors and not describing ILL experiments\n",
    "<br>\n",
    "Type 4: Articles citing the ILL, no ILL authors\n",
    "<br>\n",
    "This 'Type' information willl be included alongside the other metadata in the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all ILL publication files\n",
    "pub_ill_type1=pd.read_csv('{insert path here}/Datasets/ILL/Publications with ILL authors and describing ILL experiment.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "pub_ill_type2=pd.read_csv('{insert path here}/Datasets/ILL/Publications without ILL author and describing an ILL experiment.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "pub_ill_type3=pd.read_csv('{insert path here}/Datasets/ILL/Publications with ILL authors and not describing ILL experiment.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "pub_ill_type4=pd.read_csv('{insert path here}/Datasets/ILL/Articles citing the ILL, no ILL author.txt',sep='\t', encoding='latin-1',skiprows=1,header=0)    # Import file\n",
    "\n",
    "# Add 'Type' column\n",
    "pub_ill_type1['Type']=1\n",
    "pub_ill_type2['Type']=2\n",
    "pub_ill_type3['Type']=3\n",
    "pub_ill_type4['Type']=4\n",
    "\n",
    "# Concatenate all four types\n",
    "pub_ill=pd.concat([pub_ill_type1,pub_ill_type2,pub_ill_type3,pub_ill_type4],ignore_index=True)\n",
    "\n",
    "# Add 'Facility repository' column\n",
    "pub_ill['Facility repository']='ILL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and concat all DLS publications\n",
    "pub_dls=pd.concat([pd.read_csv('{insert path here}/Datasets/DLS/DLS annual review highlight.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS book chapter.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS conference paper.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS editor note.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS journal paper (2002-2010).csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS journal paper (2011-2020).csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS journal paper (2021-2024).csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS magazine article.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS poster.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS report.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS science highlight.csv'),pd.read_csv('{insert path here}/Datasets/DLS/DLS thesis.csv')],ignore_index=True)\n",
    "\n",
    "# Add 'Facility repository' column\n",
    "pub_dls['Facility repository']='DLS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HZB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for metadata in year 1981...\n",
      "Metadata for year 1981 saved to Datasets/HZB/metadata_1981.txt\n",
      "Searching for metadata in year 1982...\n",
      "Metadata for year 1982 saved to Datasets/HZB/metadata_1982.txt\n",
      "Searching for metadata in year 1983...\n",
      "Metadata for year 1983 saved to Datasets/HZB/metadata_1983.txt\n",
      "Searching for metadata in year 1984...\n",
      "Metadata for year 1984 saved to Datasets/HZB/metadata_1984.txt\n",
      "Searching for metadata in year 1985...\n",
      "Metadata for year 1985 saved to Datasets/HZB/metadata_1985.txt\n",
      "Searching for metadata in year 1986...\n",
      "Metadata for year 1986 saved to Datasets/HZB/metadata_1986.txt\n",
      "Searching for metadata in year 1987...\n",
      "Metadata for year 1987 saved to Datasets/HZB/metadata_1987.txt\n",
      "Searching for metadata in year 1988...\n",
      "Metadata for year 1988 saved to Datasets/HZB/metadata_1988.txt\n",
      "Searching for metadata in year 1989...\n",
      "Metadata for year 1989 saved to Datasets/HZB/metadata_1989.txt\n",
      "Searching for metadata in year 1990...\n",
      "Metadata for year 1990 saved to Datasets/HZB/metadata_1990.txt\n",
      "Searching for metadata in year 1991...\n",
      "Metadata for year 1991 saved to Datasets/HZB/metadata_1991.txt\n",
      "Searching for metadata in year 1992...\n",
      "Metadata for year 1992 saved to Datasets/HZB/metadata_1992.txt\n",
      "Searching for metadata in year 1993...\n",
      "Metadata for year 1993 saved to Datasets/HZB/metadata_1993.txt\n",
      "Searching for metadata in year 1994...\n",
      "Metadata for year 1994 saved to Datasets/HZB/metadata_1994.txt\n",
      "Searching for metadata in year 1995...\n",
      "Metadata for year 1995 saved to Datasets/HZB/metadata_1995.txt\n",
      "Searching for metadata in year 1996...\n",
      "Metadata for year 1996 saved to Datasets/HZB/metadata_1996.txt\n",
      "Searching for metadata in year 1997...\n",
      "Metadata for year 1997 saved to Datasets/HZB/metadata_1997.txt\n",
      "Searching for metadata in year 1998...\n",
      "Metadata for year 1998 saved to Datasets/HZB/metadata_1998.txt\n",
      "Searching for metadata in year 1999...\n",
      "Metadata for year 1999 saved to Datasets/HZB/metadata_1999.txt\n",
      "Searching for metadata in year 2000...\n",
      "Metadata for year 2000 saved to Datasets/HZB/metadata_2000.txt\n",
      "Searching for metadata in year 2001...\n",
      "Metadata for year 2001 saved to Datasets/HZB/metadata_2001.txt\n",
      "Searching for metadata in year 2002...\n",
      "Metadata for year 2002 saved to Datasets/HZB/metadata_2002.txt\n",
      "Searching for metadata in year 2003...\n",
      "Metadata for year 2003 saved to Datasets/HZB/metadata_2003.txt\n",
      "Searching for metadata in year 2004...\n",
      "Metadata for year 2004 saved to Datasets/HZB/metadata_2004.txt\n",
      "Searching for metadata in year 2005...\n",
      "Metadata for year 2005 saved to Datasets/HZB/metadata_2005.txt\n",
      "Searching for metadata in year 2006...\n",
      "Metadata for year 2006 saved to Datasets/HZB/metadata_2006.txt\n",
      "Searching for metadata in year 2007...\n",
      "Metadata for year 2007 saved to Datasets/HZB/metadata_2007.txt\n",
      "Searching for metadata in year 2008...\n",
      "Metadata for year 2008 saved to Datasets/HZB/metadata_2008.txt\n",
      "Searching for metadata in year 2009...\n",
      "Metadata for year 2009 saved to Datasets/HZB/metadata_2009.txt\n",
      "Searching for metadata in year 2010...\n",
      "Metadata for year 2010 saved to Datasets/HZB/metadata_2010.txt\n",
      "Searching for metadata in year 2011...\n",
      "Metadata for year 2011 saved to Datasets/HZB/metadata_2011.txt\n",
      "Searching for metadata in year 2012...\n",
      "Metadata for year 2012 saved to Datasets/HZB/metadata_2012.txt\n",
      "Searching for metadata in year 2013...\n",
      "Metadata for year 2013 saved to Datasets/HZB/metadata_2013.txt\n",
      "Searching for metadata in year 2014...\n",
      "Metadata for year 2014 saved to Datasets/HZB/metadata_2014.txt\n",
      "Searching for metadata in year 2015...\n",
      "Metadata for year 2015 saved to Datasets/HZB/metadata_2015.txt\n",
      "Searching for metadata in year 2016...\n",
      "Metadata for year 2016 saved to Datasets/HZB/metadata_2016.txt\n",
      "Searching for metadata in year 2017...\n",
      "Metadata for year 2017 saved to Datasets/HZB/metadata_2017.txt\n",
      "Searching for metadata in year 2018...\n",
      "Metadata for year 2018 saved to Datasets/HZB/metadata_2018.txt\n",
      "Searching for metadata in year 2019...\n",
      "Metadata for year 2019 saved to Datasets/HZB/metadata_2019.txt\n",
      "Searching for metadata in year 2020...\n",
      "Metadata for year 2020 saved to Datasets/HZB/metadata_2020.txt\n",
      "Searching for metadata in year 2021...\n",
      "Metadata for year 2021 saved to Datasets/HZB/metadata_2021.txt\n",
      "Searching for metadata in year 2022...\n",
      "Metadata for year 2022 saved to Datasets/HZB/metadata_2022.txt\n",
      "Searching for metadata in year 2023...\n",
      "Metadata for year 2023 saved to Datasets/HZB/metadata_2023.txt\n",
      "Searching for metadata in year 2024...\n",
      "Metadata for year 2024 saved to Datasets/HZB/metadata_2024.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Base URL for the form submission\n",
    "# search_url = \"https://www.helmholtz-berlin.de/pubbin/publikationen.pl\"\n",
    "# search_url = \"https://www.helmholtz-berlin.de/pubbin/publikationen.pl\"\n",
    "\n",
    "# Years for which to download metadata\n",
    "years = list(range(1981, 2025))  # Adjust the range of years as needed\n",
    "\n",
    "# Directory to save metadata\n",
    "save_directory = 'Datasets/HZB'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Function to extract metadata for a specific year\n",
    "def extract_metadata_by_year(year):\n",
    "    print(f\"Searching for metadata in year {year}...\")\n",
    "    \n",
    "    # Form data to submit for the search\n",
    "    form_data = {\n",
    "        'jahr': str(year),  # Year\n",
    "        'JOB': 'start search',  # Start the search\n",
    "        'sprache':'en',\n",
    "        'typ_1':'1',\n",
    "        'typ_2':'1',\n",
    "        'typ_3':'1',\n",
    "        'typ_5':'1',\n",
    "    }\n",
    "\n",
    "    # Submit the form with a POST request\n",
    "    response = requests.post(search_url, data=form_data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Failed to retrieve results for year {year}\")\n",
    "        return\n",
    "\n",
    "    # Parse the response HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the metadata section (you'll need to inspect the HTML structure to find the right tag)\n",
    "    # For example, it might be in a specific <div>, <table>, or other tag\n",
    "    metadata_text = soup.get_text()  # Get all the text on the page\n",
    "    # Alternatively, you might want to focus on specific areas using soup.find() or soup.select()\n",
    "\n",
    "    # Save the metadata to a text file\n",
    "    metadata_filename = os.path.join(save_directory, f'metadata_{year}.txt')\n",
    "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(metadata_text)\n",
    "    \n",
    "    print(f\"Metadata for year {year} saved to {metadata_filename}\")\n",
    "\n",
    "# Main loop to iterate over years\n",
    "for year in years:\n",
    "    extract_metadata_by_year(year)\n",
    "    time.sleep(5)  # Add a delay between requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract relevant information from a text file\n",
    "def extract_publications(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Regex pattern to capture text between 'select none' and 'Export as'\n",
    "    pattern = re.compile(r'select none(.*?)Export as', re.DOTALL)\n",
    "\n",
    "    # Search for the pattern\n",
    "    match = pattern.search(content)\n",
    "\n",
    "    if match:\n",
    "        # Extract the relevant section\n",
    "        relevant_section = match.group(1)\n",
    "\n",
    "        # Clean up the extracted section by stripping unnecessary whitespace\n",
    "        relevant_section = relevant_section.strip()\n",
    "\n",
    "        # # Remove multiple blank lines using regex\n",
    "        # relevant_section = re.sub(r'\\n\\s*\\n+', '\\n', relevant_section)\n",
    "\n",
    "        # Split the cleaned section into lines\n",
    "        lines = relevant_section.splitlines()\n",
    "\n",
    "        # publications = []\n",
    "        # current_entry = []\n",
    "\n",
    "        # # Process each line to group entries\n",
    "        # for line in lines:\n",
    "        #     stripped_line = line.strip()\n",
    "        #     if stripped_line:  # Only process non-empty lines\n",
    "        #         current_entry.append(stripped_line)\n",
    "\n",
    "        #         # Heuristic: Check if the line indicates the end of an entry\n",
    "        #         # For example, if it contains a DOI or a specific pattern.\n",
    "        #         if re.search(r'\\d{4}', stripped_line) or \"doi:\" in stripped_line.lower():\n",
    "        #             # This assumes the entry ends after a line containing a year or DOI\n",
    "        #             publications.append(\" \".join(current_entry))\n",
    "        #             current_entry = []  # Reset for the next entry\n",
    "\n",
    "        # # Handle any remaining lines in current_entry\n",
    "        # if current_entry:\n",
    "        #     publications.append(\" \".join(current_entry))\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    return lines\n",
    "        \n",
    "\n",
    "# file_path = 'Datasets/HZB/metadata_2016.txt'  # Replace with your file path\n",
    "# lines = extract_publications(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all HZB files\n",
    "publications_hzb=[]\n",
    "for i in range(1981,2025):\n",
    "    file_path = 'Datasets/HZB/metadata_'+str(i)+'.txt'  # Replace with your file path\n",
    "    lines = extract_publications(file_path)\n",
    "    publications_hzb=publications_hzb+lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all DOIs from publications_hzb \n",
    "doi_pattern=r\"doi:\\s(?P<doi>\\S+)(?=Open|$)\"   # DOI starts with \"doi:\" followed by a string\n",
    "hzb_doi=[]\n",
    "for entry in publications_hzb:\n",
    "    doi=re.search(doi_pattern,entry)\n",
    "    if doi:\n",
    "        hzb_doi.append(doi.group(1))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Proposals \n",
    "Now, that we have the publications, the next step is to get the proposals.\\\n",
    "For ESRF, this can be done through the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESRF Proposals\n",
    "The ESRF publications imported above are linked to their corresponding proposals through the proposal number.\\\n",
    "We want to get a complete list of ESRF proposals and map them to the corresponding papers.\\\n",
    "https://icatplus.esrf.fr/api/Documents provides a list of experimental sessions, which can then be mapped to the associated proposals and them to the assosicated publications.\\\n",
    "Note that multiple experimental sessions can be associated with a single proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ESRF publications\n",
    "\n",
    "# pub_esrf=pd.read_csv('{insert pathname}/Datasets/ESRF/Publications_ESRF')    # Import file\n",
    "pub_esrf=pd.read_csv('/Users/fdp54928/Library/CloudStorage/OneDrive-Nexus365/GitHub Repositories/synchrotron-proposals-topic-classification/Datasets/ESRF/Publications_ESRF')    # Import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any publications without DOIs\n",
    "\n",
    "pub_esrf=pub_esrf[pub_esrf['DOI'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of experimental session documents\n",
    "api_request=\"https://icatplus.esrf.fr/api/Documents\"\n",
    "response=requests.get(api_request).json()\n",
    "documents_esrf=response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DOI, Title, and Summary (abstract) information from documents_esrf into session_esrf\n",
    "# session_esrf will be a list of dicts, with each index in the list representing a proposal; dict keys are: 'doi', 'title', 'summary'\n",
    "\n",
    "session_esrf=[]        # Initialise empty session_esrf list\n",
    "\n",
    "for document in documents_esrf:\n",
    "    session_dict={}        # Initialise empty session_dict dictionary\n",
    "    \n",
    "    # Check for DOI and append those with one\n",
    "    if document['doi']!=None:\n",
    "        session_dict['doi']=document['doi']\n",
    "        session_dict['title']=document['title']\n",
    "        session_dict['summary']=document['summary']\n",
    "        session_esrf.append(session_dict)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch Proposal number information of a single session using API call\n",
    "\n",
    "def fetch_data(session):\n",
    "    api_request=\"https://icatplus.esrf.fr/doi/\"\n",
    "    response2 = requests.get(api_request+session['doi']+'/reports')\n",
    "\n",
    "    # Check for valid API call status code\n",
    "    if response2.status_code==200:\n",
    "        reports_esrf=response2.json()\n",
    "        report_esrf=reports_esrf[0]\n",
    "        session['proposal']=report_esrf['proposal']\n",
    "        return session     # prop with proposal number added \n",
    "    else:\n",
    "        return 0        # return 0 is API call result is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each session in session_esrf and apply the fetch_data function. Only append session that has valid proposal number to session_esrf_valid\n",
    "\n",
    "session_esrf_valid=[]\n",
    "for session in session_esrf:\n",
    "    fetched_data=fetch_data(session)\n",
    "    if fetched_data!=0:\n",
    "        session_esrf_valid.append(session)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Regular Expressions to match proposal number in session_esrf_valid to pub_esrf_valid\n",
    "\n",
    "n = len(session_esrf_valid)\n",
    "for i in range(n):\n",
    "    if 'proposal' in session_esrf_valid[i]:\n",
    "        pattern= r'(?i)'+session_esrf_valid[i]['proposal']\n",
    "        match = pub_esrf[pub_esrf['Proposal number'].notna()]['Proposal number'].str.match(pattern)\n",
    "        session_esrf_valid[i]['publications DOI']=pub_esrf[pub_esrf['Proposal number'].notna()][match]['DOI'].tolist()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert session_esrf_valid to DataFrame, drop session DOI column, apply upper case to all proposal names for standardisation, drop proposal duplicates (recall that a single proposal can be linked to multiple sessions)\n",
    "df=pd.DataFrame(session_esrf_valid)\n",
    "df.drop(columns='doi', inplace=True)\n",
    "df['proposal']=df['proposal'].str.upper()\n",
    "df.drop_duplicates('proposal',inplace=True)\n",
    "\n",
    "#  Export DataFrame to json file\n",
    "df.to_json('{insert pathname here}/Datasets/ESRF/Proposals_ESRF',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OpenAlex citation metadata for proposals\n",
    "Now that we have the proposals with the DOIs of the publications linked to the proposals, we want to use the OpenAlex API call on the DOIs to get the corresponding OpenAlex IDs. This is because the topic classification model takes in OpenAlex IDs and not DOIs as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ESRF proposals\n",
    "prop_esrf=pd.read_json('{insert pathname here}/Datasets/ESRF/Proposals_ESRF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>proposal</th>\n",
       "      <th>publications DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mx415</td>\n",
       "      <td>TEST</td>\n",
       "      <td>MX-415</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mx1937</td>\n",
       "      <td>Munich Crystallography BAG</td>\n",
       "      <td>MX-1937</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mx1888</td>\n",
       "      <td>IBS BAG</td>\n",
       "      <td>MX-1888</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mx1936</td>\n",
       "      <td>Cambridge MRC Block allocation</td>\n",
       "      <td>MX-1936</td>\n",
       "      <td>[10.1038/nature25462]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mx1944</td>\n",
       "      <td>FRANKFURT/HOMBURG BAG: ATOMIC MECHANISMS OF AC...</td>\n",
       "      <td>MX-1944</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>Exploring Stability of Hydrogen-filled Ices fo...</td>\n",
       "      <td>Gas hydrates have received considerable attent...</td>\n",
       "      <td>HC-5914</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6912</th>\n",
       "      <td>Analysis of the antiferromagnetic structure in...</td>\n",
       "      <td>The antiferromagnetic structure of the Au-Al-T...</td>\n",
       "      <td>IH-HC-4055</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>Investigation of the healing potential of AlMg...</td>\n",
       "      <td>In industry applications, parts are subjecte...</td>\n",
       "      <td>MA-6418</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>Investigating the Strain Variation in SiGeSn</td>\n",
       "      <td>Quantum Devices are often manufactured using s...</td>\n",
       "      <td>IH-MA-550</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>Pressure and rate dependency of phase evolutio...</td>\n",
       "      <td>The main objective is to investigate the phase...</td>\n",
       "      <td>A31-1-265</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4456 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                                 mx415   \n",
       "1                                                mx1937   \n",
       "2                                                mx1888   \n",
       "3                                                mx1936   \n",
       "4                                                mx1944   \n",
       "...                                                 ...   \n",
       "6911  Exploring Stability of Hydrogen-filled Ices fo...   \n",
       "6912  Analysis of the antiferromagnetic structure in...   \n",
       "6914  Investigation of the healing potential of AlMg...   \n",
       "6915       Investigating the Strain Variation in SiGeSn   \n",
       "6917  Pressure and rate dependency of phase evolutio...   \n",
       "\n",
       "                                                summary    proposal  \\\n",
       "0                                                  TEST      MX-415   \n",
       "1                            Munich Crystallography BAG     MX-1937   \n",
       "2                                               IBS BAG     MX-1888   \n",
       "3                        Cambridge MRC Block allocation     MX-1936   \n",
       "4     FRANKFURT/HOMBURG BAG: ATOMIC MECHANISMS OF AC...     MX-1944   \n",
       "...                                                 ...         ...   \n",
       "6911  Gas hydrates have received considerable attent...     HC-5914   \n",
       "6912  The antiferromagnetic structure of the Au-Al-T...  IH-HC-4055   \n",
       "6914    In industry applications, parts are subjecte...     MA-6418   \n",
       "6915  Quantum Devices are often manufactured using s...   IH-MA-550   \n",
       "6917  The main objective is to investigate the phase...   A31-1-265   \n",
       "\n",
       "           publications DOI  \n",
       "0                        []  \n",
       "1                        []  \n",
       "2                        []  \n",
       "3     [10.1038/nature25462]  \n",
       "4                        []  \n",
       "...                     ...  \n",
       "6911                     []  \n",
       "6912                     []  \n",
       "6914                     []  \n",
       "6915                     []  \n",
       "6917                     []  \n",
       "\n",
       "[4456 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_esrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, config\n",
    "config.email = 'terence.tan@wadham.ox.ac.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applying the API call to each row of the proposal DataFrame prop_esrf\n",
    "\n",
    "def get_openalex_id(row):\n",
    "    # Get the 'publications DOI' column from the row\n",
    "    publications_doi = row['publications DOI']\n",
    "    openalex_id = []\n",
    "    \n",
    "    for doi in publications_doi:\n",
    "        # Assume Works() is properly imported and accessible\n",
    "        result = Works().filter(doi=doi).select('id').get()\n",
    "        # If the result is not empty, append the 'id', otherwise append 0\n",
    "        if result:\n",
    "            openalex_id.append(result[0]['id'])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    row['openalex_ids']=openalex_id\n",
    "   \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the propsals with publications and those without\n",
    "\n",
    "prop_esrf_non_empty=prop_esrf[prop_esrf['publications DOI'].str.len()!=0]\n",
    "prop_esrf_empty=prop_esrf[prop_esrf['publications DOI'].str.len()==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the get_openalex_id to each row of prop_esrf_non_empty\n",
    "prop_esrf_non_empty=prop_esrf_non_empty.apply(get_openalex_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proposals with different number of publications DOI and OpenAlex IDs:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "# Check if the number of publications DOI is equal to the number of OpenAlex IDs for each proposal\n",
    "\n",
    "print(\"Number of proposals with different number of publications DOI and OpenAlex IDs:\")\n",
    "print(len(prop_esrf_non_empty[prop_esrf_non_empty['publications DOI'].apply(len)!=prop_esrf_non_empty['openalex_ids'].apply(len)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a mismatch for one of the proposals, so we investigate why this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>proposal</th>\n",
       "      <th>publications DOI</th>\n",
       "      <th>openalex_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>Release of metals and dissolution of mineral f...</td>\n",
       "      <td>Pathogenic mechanisms of absestos-related dise...</td>\n",
       "      <td>LS-3076</td>\n",
       "      <td>[10.13133/2239-1002/18090]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2825  Release of metals and dissolution of mineral f...   \n",
       "\n",
       "                                                summary proposal  \\\n",
       "2825  Pathogenic mechanisms of absestos-related dise...  LS-3076   \n",
       "\n",
       "                publications DOI openalex_ids  \n",
       "2825  [10.13133/2239-1002/18090]           []  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_esrf_non_empty[prop_esrf_non_empty['publications DOI'].apply(len)!=prop_esrf_non_empty['openalex_ids'].apply(len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular publication DOI (10.13133/2239-1002/18090) does not exist in the OpenAlex data repository. We shall move this proposal from prop_esrf_non_empty to prop_esrf_empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Row 2825 to prop_esrf_empty\n",
    "prop_esrf_empty=pd.concat([prop_esrf_empty,prop_esrf_non_empty.loc[2825:2826]])\n",
    "\n",
    "# Drop Row 2825 from prop_esrf_non_empty\n",
    "prop_esrf_non_empty=prop_esrf_non_empty.drop(2825)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all values in the 'openalex_ids' column to an empty list\n",
    "prop_esrf_empty['openalex_ids']= np.empty((len(prop_esrf_empty), 0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the proposals appended with the OpenAlex IDs and those without\n",
    "\n",
    "prop_esrf_non_empty.to_json('{insert pathname here}/Datasets/ESRF/Proposals_ESRF_openalexID',index=True)\n",
    "prop_esrf_empty.to_json('{insert pathname here}/Datasets/ESRF/Proposals_ESRF_no_openalexID',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
